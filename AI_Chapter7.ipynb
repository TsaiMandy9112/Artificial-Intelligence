{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TsaiMandy9112/Artificial-Intelligence/blob/main/AI_Chapter7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPAOoIkomT3M"
      },
      "source": [
        "#**SimpleRNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZVih_uvmfGf",
        "outputId": "016237c1-cb8b-468d-92a5-6383e9cb03b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_xh shape: (5, 2)\n",
            "W_oo shape: (2, 2)\n",
            "b_h shape: (2,)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN\n",
        "# units: positive integer, dimensionality of the output space\n",
        "# return_sequences: Boolean.\n",
        "#   Whether to return the last output in the output sequence, or the full sequence.\n",
        "#   Default: False.\n",
        "rnn_layer = tf.keras.layers.SimpleRNN(\n",
        "    units=2, use_bias=True, activation='tanh', dropout=0,\n",
        "    return_sequences=True)\n",
        "\n",
        "# inputs: A 3D tensor, with shape [batch, timesteps, feature]\n",
        "rnn_layer.build(input_shape=(None, None, 5))\n",
        "\n",
        "w_xh, w_oo, b_h = rnn_layer.weights\n",
        "\n",
        "print('W_xh shape:', w_xh.shape)\n",
        "print('W_oo shape:', w_oo.shape)\n",
        "print('b_h shape:', b_h.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygBaY-VIzyEu"
      },
      "source": [
        "# **Manually computering the output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp2Zt4-ztaRc",
        "outputId": "5ed698c9-f6f9-4c8c-87e2-ef256a249fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1. 1. 1. 1. 1.]\n",
            " [2. 2. 2. 2. 2.]\n",
            " [3. 3. 3. 3. 3.]], shape=(3, 5), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "x_seq = tf.convert_to_tensor(\n",
        "    [[1.0]*5, [2.0]*5, [3.0]*5],\n",
        "    dtype=tf.float32)\n",
        "print(x_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwEFfzhYr5rU",
        "outputId": "04dccc62-b184-41d9-8f02-9cf4c7e5f35e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time step 0 =>\n",
            "  Input: [[1. 1. 1. 1. 1.]]\n",
            "  w_xh: <tf.Variable 'simple_rnn_cell_12/kernel:0' shape=(5, 2) dtype=float32, numpy=\n",
            "array([[-0.6200572 ,  0.7433989 ],\n",
            "       [ 0.242517  , -0.12119704],\n",
            "       [-0.38525409,  0.2638626 ],\n",
            "       [ 0.8809836 , -0.12017238],\n",
            "       [ 0.2964511 ,  0.19422936]], dtype=float32)>\n",
            "  Hidden: [[0.41464037 0.96012145]]\n",
            "  w_oo: <tf.Variable 'simple_rnn_cell_12/recurrent_kernel:0' shape=(2, 2) dtype=float32, numpy=\n",
            "array([[ 0.98796964,  0.15464693],\n",
            "       [-0.15464693,  0.9879698 ]], dtype=float32)>\n",
            "  ot: tf.Tensor([[0.41464037 0.96012145]], shape=(1, 2), dtype=float32)\n",
            "  Output(manual): [[0.39240566 0.74433106]]\n",
            "  SimpleRNN output: [0.39240566 0.74433106]\n",
            "=======================================\n",
            "Time step 1 =>\n",
            "  Input: [[2. 2. 2. 2. 2.]]\n",
            "  w_xh: <tf.Variable 'simple_rnn_cell_12/kernel:0' shape=(5, 2) dtype=float32, numpy=\n",
            "array([[-0.6200572 ,  0.7433989 ],\n",
            "       [ 0.242517  , -0.12119704],\n",
            "       [-0.38525409,  0.2638626 ],\n",
            "       [ 0.8809836 , -0.12017238],\n",
            "       [ 0.2964511 ,  0.19422936]], dtype=float32)>\n",
            "  Hidden: [[0.82928073 1.9202429 ]]\n",
            "  w_oo: <tf.Variable 'simple_rnn_cell_12/recurrent_kernel:0' shape=(2, 2) dtype=float32, numpy=\n",
            "array([[ 0.98796964,  0.15464693],\n",
            "       [-0.15464693,  0.9879698 ]], dtype=float32)>\n",
            "  ot: tf.Tensor([[1.1018571 2.7163038]], shape=(1, 2), dtype=float32)\n",
            "  Output(manual): [[0.80116504 0.99129474]]\n",
            "  SimpleRNN output: [0.80116504 0.99129474]\n",
            "=======================================\n",
            "Time step 2 =>\n",
            "  Input: [[3. 3. 3. 3. 3.]]\n",
            "  w_xh: <tf.Variable 'simple_rnn_cell_12/kernel:0' shape=(5, 2) dtype=float32, numpy=\n",
            "array([[-0.6200572 ,  0.7433989 ],\n",
            "       [ 0.242517  , -0.12119704],\n",
            "       [-0.38525409,  0.2638626 ],\n",
            "       [ 0.8809836 , -0.12017238],\n",
            "       [ 0.2964511 ,  0.19422936]], dtype=float32)>\n",
            "  Hidden: [[1.243921  2.8803642]]\n",
            "  w_oo: <tf.Variable 'simple_rnn_cell_12/recurrent_kernel:0' shape=(2, 2) dtype=float32, numpy=\n",
            "array([[ 0.98796964,  0.15464693],\n",
            "       [-0.15464693,  0.9879698 ]], dtype=float32)>\n",
            "  ot: tf.Tensor([[1.8821471 3.9836311]], shape=(1, 2), dtype=float32)\n",
            "  Output(manual): [[0.95468265 0.99930704]]\n",
            "  SimpleRNN output: [0.95468265 0.99930704]\n",
            "=======================================\n"
          ]
        }
      ],
      "source": [
        "# Convert x_seq to tensor structure\n",
        "# x_seq shape: (3, 5)\n",
        "x_seq = tf.convert_to_tensor(\n",
        "    [[1.0]*5, [2.0]*5, [3.0]*5],\n",
        "    dtype=tf.float32)\n",
        "\n",
        "# output of SimepleRNN:\n",
        "# [batch=1, timesteps=3, feature=5]\n",
        "output = rnn_layer(tf.reshape(x_seq, shape=(1, 3, 5)))\n",
        "\n",
        "## manually computing the output:\n",
        "out_man = []\n",
        "for t in range(len(x_seq)):\n",
        "  # x_seq[t] shape: (5,) need reshape to (1, 5)\n",
        "  xt = tf.reshape(x_seq[t], (1, 5))\n",
        "  print('Time step {} =>'.format(t))\n",
        "  print('  Input:', xt.numpy())\n",
        "\n",
        "  ht = tf.matmul(xt, w_xh) + b_h\n",
        "  print('  w_xh:',w_xh)\n",
        "  print('  Hidden:', ht.numpy())\n",
        "\n",
        "  if t > 0:\n",
        "    prev_o = out_man[t-1]\n",
        "  else:\n",
        "    prev_o = tf.zeros(shape=(ht.shape))\n",
        "\n",
        "  ot = ht + tf.matmul(prev_o, w_oo)\n",
        "  print('  w_oo:',w_oo)\n",
        "  print('  ot:',ot)\n",
        "\n",
        "  ot = tf.math.tanh(ot)\n",
        "  out_man.append(ot)\n",
        "  print('  Output(manual):', ot.numpy())\n",
        "  print('  SimpleRNN output:'.format(t), output[0][t].numpy())\n",
        "  print('=======================================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU-ckKgzXLPQ"
      },
      "source": [
        "# Implementing RNNs for sequence modeling in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2YCshBJvCV2",
        "outputId": "19e43769-de3e-464a-c017-46d5acb52381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1nD_ZvxO7d_VusafIwrPY_AZeFLU0s39e&export=download\n",
            "To: /content/movie_data.csv.gz\n",
            "100%|██████████| 26.5M/26.5M [00:01<00:00, 23.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "data_path = 'https://drive.google.com/u/0/uc?id=1nD_ZvxO7d_VusafIwrPY_AZeFLU0s39e&export=download'\n",
        "gdown.download(data_path, 'movie_data.csv.gz')\n",
        "print('finish')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4b3cUnXZXMiJ",
        "outputId": "6c98742f-6603-4983-bd65-90a61e335ccd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review  sentiment\n",
              "49995  OK, lets start with the best. the building. al...          0\n",
              "49996  The British 'heritage film' industry is out of...          0\n",
              "49997  I don't even know where to begin on this one. ...          0\n",
              "49998  Richard Tyler is a little boy who is scared of...          0\n",
              "49999  I waited long to watch this movie. Also becaus...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17130d0c-60c3-468b-bd59-aa7d49c2a749\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>OK, lets start with the best. the building. al...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>The British 'heritage film' industry is out of...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I don't even know where to begin on this one. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>I waited long to watch this movie. Also becaus...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17130d0c-60c3-468b-bd59-aa7d49c2a749')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17130d0c-60c3-468b-bd59-aa7d49c2a749 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17130d0c-60c3-468b-bd59-aa7d49c2a749');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "fname = 'movie_data.csv.gz'\n",
        "\n",
        "# Read IMDb Dataset\n",
        "with gzip.open(fname, 'rb') as f_in, open('movie_data.csv', 'wb') as f_out:\n",
        "  shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZKGH-59d-MQ",
        "outputId": "f72a057a-6159-4b74-aa2a-b308c21020d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(b'In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class a'\n",
            " b'rea of Belle Haven, Gree') 1\n",
            "(b'OK... so... I really like Kris Kristofferson and his usual easy going delive'\n",
            " b'ry of lines in his movie') 0\n",
            "(b'***SPOILER*** Do not read this, if you think about watching that movie, alth'\n",
            " b'ough it would be a waste') 0\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Create a dataset\n",
        "target = df.pop('sentiment')\n",
        "ds_raw = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
        "\n",
        "# inspection(檢查):\n",
        "# take(3)：取 3 次\n",
        "for ex in ds_raw.take(3):\n",
        "  tf.print(ex[0].numpy()[0][:100], ex[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9ua1P5FeSC4",
        "outputId": "8e4d3e24-66af-4a77-ac7b-e1373c04004c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ds_raw_test: 25000\n",
            "ds_raw_train_valid: 25000\n",
            "ds_raw_train: 20000\n",
            "ds_raw_valid: 5000\n"
          ]
        }
      ],
      "source": [
        "# Train/validaiton/test splits\n",
        "tf.random.set_seed(1)\n",
        "ds_raw = ds_raw.shuffle(50000, reshuffle_each_iteration=False)\n",
        "\n",
        "ds_raw_test = ds_raw.take(25000)\n",
        "ds_raw_train_valid = ds_raw.skip(25000)\n",
        "ds_raw_train = ds_raw_train_valid.take(20000)\n",
        "ds_raw_valid = ds_raw_train_valid.skip(20000)\n",
        "\n",
        "print('ds_raw_test:',len(ds_raw_test))\n",
        "print('ds_raw_train_valid:',len(ds_raw_train_valid))\n",
        "print('ds_raw_train:',len(ds_raw_train))\n",
        "print('ds_raw_valid:',len(ds_raw_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sl8KxG8hSZb",
        "outputId": "bf5673e4-ee27-4574-d483-3e76e17ce2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab-size: 87007\n"
          ]
        }
      ],
      "source": [
        "# Tokenizer and Encoder\n",
        "# tfds.features.text.Tokenizer: https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/Tokenizer\n",
        "# tfds.features.text.TokenTextEncoder: https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/TokenTextEncoder\n",
        "# Encoding sequences: keeping the last 100 items in each sequence\n",
        "\n",
        "# Step 2: find unique tokens (words)\n",
        "from collections import Counter\n",
        "\n",
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "token_counts = Counter()\n",
        "\n",
        "for example in ds_raw_train:\n",
        "  tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
        "  token_counts.update(tokens)\n",
        "\n",
        "print('Vocab-size:', len(token_counts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bZzgakdj1-B",
        "outputId": "df1e7933-a970-42ab-9eeb-992168ed69bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[232, 9, 270, 1123]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Step 3: encoding each unique token into integers\n",
        "# oov_buckets: int, the number of ints to reserve for OOV hash buckets. Tokens that are OOV will be hash-modded into a OOV bucket in encode.\n",
        "# oov_token: str, the string to use for OOV ids in decode.\n",
        "# strip_vocab: bool, whether to strip whitespace from the beginning and end of elements of vocab_list.\n",
        "encoder = tfds.deprecated.text.TokenTextEncoder(token_counts, oov_buckets=1,\n",
        "    oov_token='UNK', lowercase=False,\n",
        "    tokenizer=None, strip_vocab=True, decode_token_separator=' ')\n",
        "\n",
        "example_str = 'This is an example!'\n",
        "encoder.encode(example_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_qv0tL5nxCc",
        "outputId": "8b4dd25b-f941-476d-b3e2-74c947ec5213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence length: (24,)\n",
            "Sequence length: (179,)\n",
            "Sequence length: (262,)\n",
            "Sequence length: (535,)\n",
            "Sequence length: (130,)\n"
          ]
        }
      ],
      "source": [
        "# Step 3-A: define the function for transformation\n",
        "def encode(text_tensor, label):\n",
        "  text = text_tensor.numpy()[0]\n",
        "  encoded_text = encoder.encode(text)\n",
        "  return encoded_text, label\n",
        "\n",
        "## Step 3-B: wrap the encode function to a TF Op.\n",
        "# 使用 tf.py_function 將 encode 函數包裝起來，將它轉換為 tensorflow 運算子\n",
        "# 將影評文本編碼為整數串列的過程\n",
        "def encode_map_fn(text, label):\n",
        "  return tf.py_function(encode, inp=[text, label],\n",
        "             Tout=(tf.int64, tf.int64))\n",
        "\n",
        "ds_train = ds_raw_train.map(encode_map_fn)\n",
        "ds_valid = ds_raw_valid.map(encode_map_fn)\n",
        "ds_test = ds_raw_test.map(encode_map_fn)\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "# 顯示目前序列長度不同\n",
        "for example in ds_train.shuffle(1000).take(5):\n",
        "  print('Sequence length:', example[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6JtdG_vpdtc",
        "outputId": "d3314bda-25f9-4436-e2b2-52a63cec01f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual size: (119,)\n",
            "Individual size: (688,)\n",
            "Individual size: (308,)\n",
            "Individual size: (204,)\n",
            "Individual size: (326,)\n",
            "Individual size: (240,)\n",
            "Individual size: (127,)\n",
            "Individual size: (453,)\n",
            "Batch dimension: (4, 688)\n",
            "Batch dimension: (4, 453)\n"
          ]
        }
      ],
      "source": [
        "# Take a small subset\n",
        "# 取出大小為 8 的子集\n",
        "ds_subset = ds_train.take(8)\n",
        "for example in ds_subset:\n",
        "  print('Individual size:', example[0].shape)\n",
        "\n",
        "## batching the datasets\n",
        "# 設定 batch size = 4\n",
        "ds_batched = ds_subset.padded_batch(\n",
        "  4, padded_shapes=([-1], []))\n",
        "\n",
        "for batch in ds_batched:\n",
        "  print('Batch dimension:', batch[0].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoxXMhjsb2K3"
      },
      "outputs": [],
      "source": [
        "# batching the datasets\n",
        "train_data = ds_train.padded_batch(\n",
        "    32, padded_shapes=([-1],[]))\n",
        "\n",
        "valid_data = ds_valid.padded_batch(\n",
        "    32, padded_shapes=([-1],[]))\n",
        "\n",
        "test_data = ds_test.padded_batch(\n",
        "    32, padded_shapes=([-1],[]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4uWwfLQc4lU",
        "outputId": "84c13900-57e5-4c97-ecb2-cca0fa9f1969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embed-layer (Embedding)     (None, 20, 6)             600       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 600\n",
            "Trainable params: 600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Embedding layers for sentence encoding\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Embedding(input_dim=100,\n",
        "          output_dim=6,\n",
        "          input_length=20,\n",
        "          name='embed-layer'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "392PMlZ9iVCm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1PxTSYefdiz",
        "outputId": "fdfd60af-ab88-4b61-edcd-e0758cdb4ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          320000    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, None, 32)          2080      \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 32)                2080      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 324,193\n",
            "Trainable params: 324,193\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# An example of building a RNN model\n",
        "# with SimpleRNN layer\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNu0PoVqg3Sl",
        "outputId": "939de639-e8f3-42ca-a6bb-0c566003ad7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 32)          320000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 32)          8320      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 32)                8320      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 336,673\n",
            "Trainable params: 336,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# An example of building a RNN model\n",
        "# with LSTM layer\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erElI90ShxFs",
        "outputId": "c20f5d2c-8939-4f2d-b2fc-34b0a5b6fa39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, None, 32)          320000    \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (None, None, 32)          6336      \n",
            "                                                                 \n",
            " gru_3 (GRU)                 (None, 32)                6336      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 332,705\n",
            "Trainable params: 332,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# An example of building a RNN model\n",
        "# with GRU layer\n",
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(GRU(32, return_sequences=True))\n",
        "model.add(GRU(32))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2zsVbOijHRU",
        "outputId": "ee602717-828f-4339-cf72-75878f993d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embed-layer (Embedding)     (None, None, 32)          2784288   \n",
            "                                                                 \n",
            " bidir-lstm (Bidirectional)  (None, 128)               49664     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,842,273\n",
            "Trainable params: 2,842,273\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 32\n",
        "vocab_size = len(token_counts) + 2\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# build the model\n",
        "bi_lstm_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        name='embed-layer'),\n",
        "\n",
        "    tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(64, name='lstm-layer'),\n",
        "        name='bidir-lstm'),\n",
        "\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "bi_lstm_model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile and train:\n",
        "bi_lstm_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = bi_lstm_model.fit(\n",
        "    train_data,\n",
        "    validation_data=valid_data,\n",
        "    epochs=10)\n",
        "\n",
        "# evaluate on the test data\n",
        "test_results= bi_lstm_model.evaluate(test_data)\n",
        "print('Test Acc.: {:.2f}%'.format(test_results[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-_neHiDCDna",
        "outputId": "296a6a72-d38a-40b2-c7fa-9587d7fdc824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 386s 612ms/step - loss: 0.4895 - accuracy: 0.7603 - val_loss: 0.4793 - val_accuracy: 0.7728\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 374s 599ms/step - loss: 0.2631 - accuracy: 0.9004 - val_loss: 0.4225 - val_accuracy: 0.8600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv822gNLksoP"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('models'):\n",
        "    os.mkdir('models')\n",
        "\n",
        "bi_lstm_model.save('Bidir-LSTM-full-length-seq.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMZUWqeG33D8"
      },
      "source": [
        "# Trying SimpleRNN with short sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H6yootRkFfj"
      },
      "outputs": [],
      "source": [
        "# Trying SimpleRNN with short sequences\n",
        "def preprocess_datasets(\n",
        "  ds_raw_train,\n",
        "  ds_raw_valid,\n",
        "  ds_raw_test,\n",
        "  max_seq_length=None,\n",
        "  batch_size=32):\n",
        "\n",
        "  # Step 1: (already done => creating a dataset)\n",
        "  # Step 2: find unique tokens\n",
        "  tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "  token_counts = Counter()\n",
        "\n",
        "  for example in ds_raw_train:\n",
        "    tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
        "    if max_seq_length is not None:\n",
        "      tokens = tokens[-max_seq_length:]\n",
        "    token_counts.update(tokens)\n",
        "\n",
        "  print('Vocab-size:', len(token_counts))\n",
        "\n",
        "  # Step 3: encoding the texts\n",
        "  encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
        "  def encode(text_tensor, label):\n",
        "    text = text_tensor.numpy()[0]\n",
        "    encoded_text = encoder.encode(text)\n",
        "    if max_seq_length is not None:\n",
        "      encoded_text = encoded_text[-max_seq_length:]\n",
        "    return encoded_text, label\n",
        "\n",
        "  def encode_map_fn(text, label):\n",
        "    return tf.py_function(encode, inp=[text, label],\n",
        "                Tout=(tf.int64, tf.int64))\n",
        "\n",
        "  ds_train = ds_raw_train.map(encode_map_fn)\n",
        "  ds_valid = ds_raw_valid.map(encode_map_fn)\n",
        "  ds_test = ds_raw_test.map(encode_map_fn)\n",
        "\n",
        "  # Step 4: batching the datasets\n",
        "  train_data = ds_train.padded_batch(\n",
        "    batch_size, padded_shapes=([-1],[]))\n",
        "\n",
        "  valid_data = ds_valid.padded_batch(\n",
        "    batch_size, padded_shapes=([-1],[]))\n",
        "\n",
        "  test_data = ds_test.padded_batch(\n",
        "    batch_size, padded_shapes=([-1],[]))\n",
        "\n",
        "  return (train_data, valid_data,\n",
        "      test_data, len(token_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw7-scpY304r"
      },
      "outputs": [],
      "source": [
        "def build_rnn_model(embedding_dim, vocab_size,\n",
        "          recurrent_type='SimpleRNN',\n",
        "          n_recurrent_units=64,\n",
        "          n_recurrent_layers=1,\n",
        "          bidirectional=True):\n",
        "\n",
        "  tf.random.set_seed(1)\n",
        "  # build the model\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(\n",
        "    Embedding(input_dim=vocab_size,\n",
        "         output_dim=embedding_dim,\n",
        "         name='embed-layer')\n",
        "    )\n",
        "\n",
        "  for i in range(n_recurrent_layers):\n",
        "    return_sequences = (i < n_recurrent_layers-1)\n",
        "\n",
        "    if recurrent_type == 'SimpleRNN':\n",
        "      recurrent_layer = SimpleRNN(\n",
        "            units=n_recurrent_units,\n",
        "            return_sequences=return_sequences,\n",
        "            name='simprnn-layer-{}'.format(i))\n",
        "    elif recurrent_type == 'LSTM':\n",
        "            recurrent_layer = LSTM(\n",
        "            units=n_recurrent_units,\n",
        "            return_sequences=return_sequences,\n",
        "            name='lstm-layer-{}'.format(i))\n",
        "    elif recurrent_type == 'GRU':\n",
        "            recurrent_layer = GRU(\n",
        "            units=n_recurrent_units,\n",
        "            return_sequences=return_sequences,\n",
        "            name='gru-layer-{}'.format(i))\n",
        "\n",
        "    if bidirectional:\n",
        "        recurrent_layer = Bidirectional(\n",
        "          recurrent_layer, name='bidir-'+recurrent_layer.name)\n",
        "\n",
        "    model.add(recurrent_layer)\n",
        "  model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W8Bv_tH8mvy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "batch_size = 32\n",
        "embedding_dim = 20\n",
        "max_seq_length = 100\n",
        "\n",
        "train_data, valid_data, test_data, n = preprocess_datasets(\n",
        "  ds_raw_train, ds_raw_valid, ds_raw_test,\n",
        "  max_seq_length=max_seq_length,\n",
        "  batch_size=batch_size\n",
        ")\n",
        "\n",
        "# index = 0 for padding placeholder\n",
        "# index = n + 1 for oov\n",
        "vocab_size = n + 2\n",
        "\n",
        "rnn_model = build_rnn_model(\n",
        "  embedding_dim, vocab_size,\n",
        "  recurrent_type='SimpleRNN',\n",
        "  n_recurrent_units=64,\n",
        "  n_recurrent_layers=1,\n",
        "  bidirectional=True)\n",
        "\n",
        "rnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXV3hI66CnJC"
      },
      "outputs": [],
      "source": [
        "rnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "         loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "         metrics=['accuracy'])\n",
        "\n",
        "history = rnn_model.fit(train_data, validation_data=valid_data, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ebz77h8FFgG"
      },
      "outputs": [],
      "source": [
        "results = rnn_model.evaluate(test_data)\n",
        "print('Test Acc.: {:.2f}%'.format(results[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aQthD2eO-zE"
      },
      "source": [
        "# 利用Keras建構LSTM模型，以Stock Prediction 為例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xcV2vU_mskB"
      },
      "outputs": [],
      "source": [
        "# 來源：https://reurl.cc/OXaEE7\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 連接 google 雲端硬碟\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "fname = '/content/drive/MyDrive/AI/SPY.csv'\n",
        "# 連接 google 雲端硬碟\n",
        "\n",
        "def readTrain():\n",
        "  train = pd.read_csv(fname)\n",
        "  return train\n",
        "\n",
        "# 切割時間，增加Features，例如星期幾、幾月、幾號等。\n",
        "def augFeatures(train):\n",
        "  train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
        "  train[\"year\"] = train[\"Date\"].dt.year\n",
        "  train[\"month\"] = train[\"Date\"].dt.month\n",
        "  train[\"date\"] = train[\"Date\"].dt.day\n",
        "  train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
        "  return train\n",
        "\n",
        "# 將所有資料做正規化，而由於Date是字串非數字，因此先將它drop掉\n",
        "def normalize(train):\n",
        "  train = train.drop([\"Date\"], axis=1)\n",
        "  train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
        "  return train_norm\n",
        "\n",
        "# Build Training Data\n",
        "def buildTrain(train, pastDay=30, futureDay=5):\n",
        "  X_train, Y_train = [], []\n",
        "  for i in range(train.shape[0]-futureDay-pastDay):\n",
        "    X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
        "    Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Adj Close\"]))\n",
        "  return np.array(X_train), np.array(Y_train)\n",
        "\n",
        "# 打亂順序\n",
        "def shuffle(X,Y):\n",
        "  np.random.seed(10)\n",
        "  randomList = np.arange(X.shape[0])\n",
        "  np.random.shuffle(randomList)\n",
        "  return X[randomList], Y[randomList]\n",
        "\n",
        "# Training data & Validation data\n",
        "def splitData(X,Y,rate):\n",
        "  X_train = X[int(X.shape[0]*rate):]\n",
        "  Y_train = Y[int(Y.shape[0]*rate):]\n",
        "  X_val = X[:int(X.shape[0]*rate)]\n",
        "  Y_val = Y[:int(Y.shape[0]*rate)]\n",
        "  return X_train, Y_train, X_val, Y_val\n",
        "\n",
        "# 模型建置\n",
        "# 一對一\n",
        "def buildOneToOneModel(shape):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
        "  # output shape: (1, 1)\n",
        "  model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
        "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "  model.summary()\n",
        "  return model\n",
        "# 多對一\n",
        "def buildManyToOneModel(shape):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
        "  # output shape: (1, 1)\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "  model.summary()\n",
        "  return model\n",
        "# 一對多\n",
        "def buildOneToManyModel(shape):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
        "  # output shape: (5, 1)\n",
        "  model.add(Dense(1))\n",
        "  model.add(RepeatVector(5))\n",
        "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "  model.summary()\n",
        "  return model\n",
        "# 多對多\n",
        "def buildManyToManyModel(shape):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=True))\n",
        "  # output shape: (5, 1)\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "# 一對一訓練過程\n",
        "# read SPY.csv\n",
        "train = readTrain()\n",
        "\n",
        "# Augment the features (year, month, date, day)\n",
        "train_Aug = augFeatures(train)\n",
        "\n",
        "# Normalization\n",
        "train_norm = normalize(train_Aug)\n",
        "\n",
        "# build Data, use last 1 days to predict next 1 days\n",
        "X_train, Y_train = buildTrain(train_norm, 1, 1)\n",
        "\n",
        "# shuffle the data, and random seed is 10\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "# split training data and validation data\n",
        "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
        "# X_trian: (5710, 30, 10)\n",
        "# Y_train: (5710, 5, 1)\n",
        "# X_val: (634, 30, 10)\n",
        "# Y_val: (634, 5, 1)\n",
        "\n",
        "# from 2 dimmension to 3 dimension\n",
        "Y_train = Y_train[:,np.newaxis]\n",
        "Y_val = Y_val[:,np.newaxis]\n",
        "\n",
        "model = buildOneToOneModel(X_train.shape)\n",
        "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
        "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n",
        "\n",
        "# 多對一訓練過程\n",
        "train = readTrain()\n",
        "train_Aug = augFeatures(train)\n",
        "train_norm = normalize(train_Aug)\n",
        "# change the last day and next day\n",
        "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
        "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
        "\n",
        "model = buildManyToOneModel(X_train.shape)\n",
        "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
        "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n",
        "\n",
        "# 一對多訓練過程\n",
        "train = readTrain()\n",
        "train_Aug = augFeatures(train)\n",
        "train_norm = normalize(train_Aug)\n",
        "# change the last day and next day\n",
        "X_train, Y_train = buildTrain(train_norm, 1, 5)\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
        "\n",
        "# from 2 dimmension to 3 dimension\n",
        "Y_train = Y_train[:,:,np.newaxis]\n",
        "Y_val = Y_val[:,:,np.newaxis]\n",
        "\n",
        "model = buildOneToManyModel(X_train.shape)\n",
        "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
        "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n",
        "\n",
        "# 多對多訓練模型\n",
        "train = readTrain()\n",
        "train_Aug = augFeatures(train)\n",
        "train_norm = normalize(train_Aug)\n",
        "# change the last day and next day\n",
        "X_train, Y_train = buildTrain(train_norm, 5, 5)\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
        "\n",
        "# from 2 dimmension to 3 dimension\n",
        "Y_train = Y_train[:,:,np.newaxis]\n",
        "Y_val = Y_val[:,:,np.newaxis]\n",
        "\n",
        "model = buildManyToManyModel(X_train.shape)\n",
        "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
        "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hhgWkxFwZ3_"
      },
      "source": [
        "# Bidirectional LSTM on IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM8EtmfswaXx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "max_features = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "\n",
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
        "    num_words=max_features\n",
        ")\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2P-IwwdxVbO"
      },
      "source": [
        "# Character-level text generation with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge0trSYfxWd4",
        "outputId": "85278887-fb0a-484e-bd9c-dc06b72692cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 0us/step\n",
            "Corpus length: 600893\n",
            "Total chars: 56\n",
            "Number of sequences: 200285\n",
            "1565/1565 [==============================] - 187s 118ms/step - loss: 2.2202\n",
            "\n",
            "Generating text after epoch: 0\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \"od psychology served not only to cast su\"\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \"od psychology served not only to cast su\"\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \"od psychology served not only to cast su\"\n",
            "...Diversity: 1.2\n",
            "...Generating with seed: \"od psychology served not only to cast su\"\n",
            "...Generated:  co\"--more mostanch so thin, thought and ever the impaan digredting, rowly, the grad at such mist!\" which muy we newly be over haivice\" does detainery accourengs quite;--nauss phich go \"tone serensiness themen but so faiturlidingated ateal war defect light knowlecs belombois.  180. advate?, only moreer, with hards spirites stoyed, \"bach the lems, antay as\" oven _ emeepces! us--beporent gor-meinnt l\n",
            "\n",
            "1565/1565 [==============================] - 182s 116ms/step - loss: 1.5684\n",
            "\n",
            "Generating text after epoch: 1\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \"ill\"--a will to the veritable, actual ne\"\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \"ill\"--a will to the veritable, actual ne\"\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \"ill\"--a will to the veritable, actual ne\"\n",
            "...Diversity: 1.2\n",
            "...Generating with seed: \"ill\"--a will to the veritable, actual ne\"\n",
            "...Generated:  tte only dlie-dounce.; here--in \"withd and fact work, rad; every more busd has diity, \"by he clo de\"p! the rin-worms hoingely but toolical aldudge conpu terinatees ir met influence-clangh! the encentmeach comman-!his, whty)e.  that sake, cliest yours is vained .uporp tome spiculing; of ochiss (or institary, havpiest- stating,\"--it hwhisd christaul avate,\"-- this whate no longer considiceinful over\n",
            "\n",
            "1565/1565 [==============================] - 183s 117ms/step - loss: 1.4805\n",
            "\n",
            "Generating text after epoch: 2\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \"disclose what it really is--namely, a wi\"\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \"disclose what it really is--namely, a wi\"\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \"disclose what it really is--namely, a wi\"\n",
            "...Diversity: 1.2\n",
            "...Generating with seed: \"disclose what it really is--namely, a wi\"\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "\n",
        "# Prepare the data\n",
        "path = keras.utils.get_file(\n",
        "    \"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
        ")\n",
        "with io.open(path, encoding=\"utf-8\") as f:\n",
        "  text = f.read().lower()\n",
        "text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
        "print(\"Corpus length:\", len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "  sentences.append(text[i : i + maxlen])\n",
        "  next_chars.append(text[i + maxlen])\n",
        "print(\"Number of sequences:\", len(sentences))\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    x[i, t, char_indices[char]] = 1\n",
        "  y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "# Build the model: a single LSTM layer\n",
        "model = keras.Sequential(\n",
        "  [\n",
        "    keras.Input(shape=(maxlen, len(chars))),\n",
        "    layers.LSTM(128),\n",
        "    layers.Dense(len(chars), activation=\"softmax\"),\n",
        "  ]\n",
        ")\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "\n",
        "# Prepare the text sampling function\n",
        "def sample(preds, temperature=1.0):\n",
        "  # helper function to sample an index from a probability array\n",
        "  preds = np.asarray(preds).astype(\"float64\")\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "# Train the model\n",
        "epochs = 40\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.fit(x, y, batch_size=batch_size, epochs=1)\n",
        "  print()\n",
        "  print(\"Generating text after epoch: %d\" % epoch)\n",
        "\n",
        "  start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "  for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "    print(\"...Diversity:\", diversity)\n",
        "\n",
        "    generated = \"\"\n",
        "    sentence = text[start_index : start_index + maxlen]\n",
        "    print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "    for i in range(400):\n",
        "      x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(sentence):\n",
        "        x_pred[0, t, char_indices[char]] = 1.0\n",
        "      preds = model.predict(x_pred, verbose=0)[0]\n",
        "      next_index = sample(preds, diversity)\n",
        "      next_char = indices_char[next_index]\n",
        "      sentence = sentence[1:] + next_char\n",
        "      generated += next_char\n",
        "\n",
        "  print(\"...Generated: \", generated)\n",
        "  print()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}